{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wprowadzenie do Apache Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-kv-ToZfwtEq"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrMaciejKowalski/BigData2022-initial-project/blob/main/colabs/Wprowadzenie_do_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Sparka\n",
        "\n",
        "## Utworzenie środowiska pyspark do obliczeń\n",
        "\n",
        "Tworzymy swoje środowisko z pysparkiem we wenętrzu naszych zasobów chmurowych"
      ],
      "metadata": {
        "id": "-kv-ToZfwtEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K-Nw9adkwbsP"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "72SlZMkxya33"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.3.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "Y-DtnLhOydSI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "k2CRzSP0ygcu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "oJCqLkLaylIX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utworzenie sesji z pyspark\n",
        "\n",
        "\n",
        "Utworzymy testowo sesję aby zobaczyć czy działa. Element ten jest wspólny również gdy systemy sparkowe pracują w sposób ciągły, a nie są tworzone przez naszą sesję."
      ],
      "metadata": {
        "id": "kIGkT1Zz4-Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "X_nydnCVy7X5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apache Spark \n",
        "\n",
        "**Apache Spark** to zunifikowany silnik do obliczeń rozproszonych na licencji open-source. Oferuje interfejs pozwalający na programowanie obliczeń na klastrach z domyślną paralelizacją oraz odpornością na awarie.\n",
        "\n",
        "Ze Sparkiem pracować można w Scali, Pythonie, Javie oraz R.\n",
        "\n",
        "Jego przewaga nad model Map-Reduce Hadoopa polega na unikaniu zapisów na hdfs tak długo jak to możliwe - i posługiwaniu się RAMem nodów jak długo go wystarcza.\n",
        "\n",
        "**Komponenty Sparka:**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/PiotrMaciejKowalski/kurs-analiza-danych-2022/main/Tydzie%C5%84%206/cluster-overview.png\" alt=\"title\" width=\"500\"/>\n",
        "\n",
        "* Spark \"core\" - podstawa Sparka z podstawową abstrakcją danych nazywaną RDD\n",
        "* Spark SQL - komponent pozwalający na operowanie na ustrukturyzowanych danych z wykorzystaniem operacji znanych z SQL - łatwy w użyciu\n",
        "* Spark MLlib - komponent zawierający algorytmy ML dostępne w Sparku - ML na skalę klastrów\n",
        "* Spark Streaming - moduł pozwalający na pracę ze strumnieniami danych\n",
        "* Spark GraphX - komponent do pracy z grafami\n",
        "\n",
        "**Architektura Sparka:**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/PiotrMaciejKowalski/kurs-analiza-danych-2022/main/Tydzie%C5%84%206/spark-stack.png\" alt=\"title\" width=\"500\"/>\n",
        "\n",
        "* driver - proces uruchamiający główną funkcję aplikacji i tworzący SparkContext\n",
        "* executor(y) - proces uruchomiony dla aplikacji w węźle roboczym (worker node), który uruchamia zadania i przechowuje dane w pamięci lub na dysku. Każda aplikacja ma własne executory\n",
        "* cluster manager - dostępne opcje: YARN, Mesos, Kubernetes, Standalone\n",
        "\n",
        "**SparkSession:**\n",
        "* wprowadzony w Spark 2.0\n",
        "* składa się ze SparkContextu, SQLContextu oraz HiveContext\n",
        "* zwykle nazywany w kodzie `spark`\n",
        "* kroki niezbędne do utworzenia SparkSession w pySparku:\n",
        "\n",
        "> from pyspark.sql import SparkSession  \n",
        "> spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "## RDD\n",
        "\n",
        "Podstawowym formatem danych (coś jak tabela w db) jest RDD. Skrót rozwija się następująco:\n",
        "* R - resilient (elastyczny)\n",
        "* D - distributed\n",
        "* D - dataset\n",
        "\n",
        "Model RDD jest napisany w sposób wspierający przekstrzałcenia Map-Reduce jako domyślny. W związku z powyższym wykazuje się następującymi własnościami:\n",
        "\n",
        "\n",
        "* immutable - każdy obiekt jest niezmienniczy. Chcesz coś zmienić - musisz utworzyć nowy rdd\n",
        "* in-memory - przetwarzany głównie w RAM\n",
        "* lazy evaluated - silnik obliczeniowy wykonuje obliczenia dopiero gdy okażą się konieczne.\n",
        "* parallel - współbieżny \n",
        "\n",
        "Z RDD stowarzyszone są dwa rodzaje czynności:\n",
        "* akcje, oraz\n",
        "* transformacje\n",
        "\n",
        "### Transformacje \n",
        "\n",
        "Modelują czynności jakie możemy chcieć wykonywać na danych. Przekształcenia (map), redukcje (reduce), filtry (filter). Mają charakter opisu skąd się biorą pewne wartości. W naszym ujęciu mogą odpowiadać funkcjom mapper, reducer i podobnym. \n",
        "\n",
        "_Dla osób, które kojarzą paradygmat funkcyjny programowania - można dodać, że transformacje dotyczą funkcji czystych._\n",
        "\n",
        "### Akcje\n",
        "\n",
        "Modelują czynności z uwagi na wynik jaki oczekujemy. Wyświetl, zapisz, wyszukaj. Mają charakter silnie połączony z wynikiem działania."
      ],
      "metadata": {
        "id": "WmcWNvu4HJTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby obliczenia na danych zostały wykonane - musi zostać uruchomiona akcja. Dopiero ona wykona odpowiednie (i tylko te konieczne) transformacje.\n",
        "\n",
        "## DataFrame\n",
        "\n",
        "Choć RDD są wszędzie w Sparku, obecnie już się ich nie widzi. Od Sparka w wersji 2.0 zostały przesłonięte nowym interfejsem (zostały spakowane do wnętrza) czegoś nazywanego Ramką Danych (Dataframe). Skojarzenie z dataframe z R lub Pandas Python jest tutaj bardzo naturalne i prawdziwe. DataFrame Sparka były na nich wzorowane i pokrywają się w dużym obszarze składni.\n",
        "\n",
        "**DataFrame:**\n",
        "* abstrakcja danych z modułu Spark SQL\n",
        "* zawiera dodatkowe informacje o strukturze danych (schema)\n",
        "* pozwala na pracę z danymi wykorzysując zapytania znane z SQL/Hive\n",
        "\n",
        "Dalej zaprezentujemy jak to się odbywa w praktyce"
      ],
      "metadata": {
        "id": "XkyMB6tsHJbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Podłączenie Google Drive do sesji colab"
      ],
      "metadata": {
        "id": "xHvcpXvTUHRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "hi4HFkt1UG59",
        "outputId": "8fca4705-e62e-42f6-a434-c45ed99ccd10"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Przykładowy processing danych w Spark\n",
        "\n",
        "## Wczytanie danych do Sparka\n",
        "\n",
        "W tej części wczytamy sobie nasz plik `flights.csv` do przetwarzania w Spark.\n",
        "\n",
        "Z uwagi na to, że nasz plik to csv bez nagłówka - trzeba zdefiniować schemat dla danych, które przetwarzamy\n",
        "\n",
        "Przypomnijmy listę pól\n",
        "YEAR,MONTH,DAY,DAY_OF_WEEK,AIRLINE,FLIGHT_NUMBER,TAIL_NUMBER,ORIGIN_AIRPORT,\\\n",
        "DESTINATION_AIRPORT,SCHEDULED_DEPARTURE,DEPARTURE_TIME,DEPARTURE_DELAY,\\\n",
        "TAXI_OUT,WHEELS_OFF,SCHEDULED_TIME,ELAPSED_TIME,AIR_TIME,DISTANCE,WHEELS_ON,\\\n",
        "TAXI_IN,SCHEDULED_ARRIVAL,ARRIVAL_TIME,ARRIVAL_DELAY,DIVERTED,CANCELLED,\\\n",
        "CANCELLATION_REASON,AIR_SYSTEM_DELAY,SECURITY_DELAY,AIRLINE_DELAY,\\\n",
        "LATE_AIRCRAFT_DELAY,WEATHER_DELAY\n",
        "\n",
        "_Sporo ich. Więc najpierw trochę magii (bo nie chce mi się kodować każdego pola ręcznie, a jestem leniwy). Ufam, że przykład pozwoli rozszerzyć zastosowanie do bardziej skomplikowanych zastosowań. Na pocieszenie dodam, że wczytywanie csv bez nagłówka to najgorszy scenariusz w wersji wczytywania w sparku._"
      ],
      "metadata": {
        "id": "AOf1bVK0XK5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pola_zbiorczo = '''YEAR,MONTH,DAY,DAY_OF_WEEK,AIRLINE,FLIGHT_NUMBER,TAIL_NUMBER,ORIGIN_AIRPORT,DESTINATION_AIRPORT,SCHEDULED_DEPARTURE,DEPARTURE_TIME,DEPARTURE_DELAY,TAXI_OUT,WHEELS_OFF,SCHEDULED_TIME,ELAPSED_TIME,AIR_TIME,DISTANCE,WHEELS_ON,TAXI_IN,SCHEDULED_ARRIVAL,ARRIVAL_TIME,ARRIVAL_DELAY,DIVERTED,CANCELLED,CANCELLATION_REASON,AIR_SYSTEM_DELAY,SECURITY_DELAY,AIRLINE_DELAY,LATE_AIRCRAFT_DELAY,WEATHER_DELAY'''\n",
        "pola = pola_zbiorczo.split(',')"
      ],
      "metadata": {
        "id": "5q3upeS7Xdb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dalej użyjemy funkcji add(Nazwa, Typ pola, czy może być null) do zapisania prostego schematu danych"
      ],
      "metadata": {
        "id": "LcPWpEe2XgnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StringType\n",
        "\n",
        "schemat = StructType()\n",
        "for pole in pola:\n",
        "    schemat = schemat.add(pole, StringType(), True)"
      ],
      "metadata": {
        "id": "dqg20HMzXgFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Przejdźmy do wczytywania"
      ],
      "metadata": {
        "id": "vYv4PqDnXldm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option(\"header\", False).schema(schemat).load('/content/drive/MyDrive/flights.csv')\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "93HG9JboXkzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "1Oi8_clWY3Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['YEAR','MONTH','DAY','AIRLINE','DISTANCE'].show()"
      ],
      "metadata": {
        "id": "OfAKsrWgUaXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "YUCjh98hY7E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.count()"
      ],
      "metadata": {
        "id": "Xa2lKAeQY9fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.show(15)"
      ],
      "metadata": {
        "id": "P5nSQTJxUFyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.show(1015)"
      ],
      "metadata": {
        "id": "qS3g8OURUF5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zapytania Spark-SQL\n",
        "\n",
        "Zapytania do Sparka kierowane są za pomocą \n",
        "\n",
        "* składni a.k.a. SQL, lub\n",
        "* wyrażone w ORM (object relational mapping) czyli obiektowym sposobie na wyrażanie kwerend.\n",
        "\n",
        "Składniowo wydaje się, że zapytania SQL są łatwiejsze do zapisania. W kilku przypadkach jednak jawne zadanie kolejności obliczeń może pomóc zoptymalizować kształt zapytania.\n",
        "\n",
        "Zaprezentuje kilka podstawowych sposób na odpytywanie Spark DataFrame kwerendami o różnych naturach. Zawsze podane obe będą w postaci SparkSQL oraz wyrażenia ORM.\n",
        "\n",
        "### Proste kwerendy\n",
        "\n",
        "Zanim zacznimy pisać kwerendy należy jeszcze dodać nasz DataFrame do 'przestrzeni nazw tabel' Sparka. Formalnie nazywane jest to widokiem danych "
      ],
      "metadata": {
        "id": "0WSmiUSyZPTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"df\")"
      ],
      "metadata": {
        "id": "nZG8gmcPY_D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wykonajmy prostego Selecta z tego zbioru.\n",
        "\n",
        "Przypominam, że do uruchomienia sparka potrzebna jest akcja. Np. taki `show()`"
      ],
      "metadata": {
        "id": "eJ4RB7KbZd32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select airline, distance from df').show()"
      ],
      "metadata": {
        "id": "vy0PQJBCZZow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('airline','distance').show()"
      ],
      "metadata": {
        "id": "8Q510rgbZcr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sprawniejsze wyświetlanie danych sparkowych"
      ],
      "metadata": {
        "id": "bsZ7C6s_dVng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('airline','distance').limit(20).toPandas()"
      ],
      "metadata": {
        "id": "tMqrI4GJZjnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proste grupowania i agregacje\n",
        "\n",
        "Dalej proste pogrupowanie z polem poddanym agregacji."
      ],
      "metadata": {
        "id": "Jq8kWjoGdVr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "spark.sql('select airline, count(*) as count from df group by airline').show()"
      ],
      "metadata": {
        "id": "m3ua5JxOeOt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.groupBy('airline').count().show()"
      ],
      "metadata": {
        "id": "foC3X34WeQBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Klauzala sortująca\n",
        "\n",
        "Możemy dane uporządkować względem kolumny"
      ],
      "metadata": {
        "id": "Vd6DdvwoeT2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "spark.sql('select airline, count(*) as count from df group by airline order by count').show()"
      ],
      "metadata": {
        "id": "HXRVONlQeRlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.groupBy('airline').count().orderBy('count').show()"
      ],
      "metadata": {
        "id": "X0WKisQQea72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy zmienić funkcje agregacji na mniej oczywistą lub zadać ich więcej.\n"
      ],
      "metadata": {
        "id": "Wq5RPs9WeghP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "spark.sql('select airline, max(distance) as maks, min(distance) as min from df group by airline').show()"
      ],
      "metadata": {
        "id": "yubp7LvMeeHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "from pyspark.sql import functions as sf #spark functions\n",
        "\n",
        "df.groupBy('airline').agg(sf.max('distance').alias('maks'), sf.min('distance').alias('min')).show()"
      ],
      "metadata": {
        "id": "exmGTe-helnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrowanie danych\n",
        "\n",
        "Bardzo ważne jest oczywiście odflitrowanie części dużego zbioru danych. \n",
        "\n",
        "_Uwaga pamiętajmy, że leniwie wczytując plik skazałem wszystkie pola na bycie Stringami._"
      ],
      "metadata": {
        "id": "wTtH38jDerZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "spark.sql('select airline, count(*) as count from df  where day_of_week = \"2\" group by airline').show()"
      ],
      "metadata": {
        "id": "GRSu3X7deoJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "df.where('day_of_week = \"2\"').groupBy('airline').count().show()"
      ],
      "metadata": {
        "id": "RRbjUi9weuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zaawansowany preprocessing danych w Spark\n",
        "\n",
        "Aby dokładniej poznać możliwości oferowane przez Sparka rozbudujemy przejrzymy listę operacji i sposoby ich wykorzystywania ponownie i dokładniej.\n",
        "\n",
        "## Perspektywa RDD\n",
        "\n",
        "RDD jest originalnym interfejsem dostępu do danych w Sparku. I w kryzysowych sytuacjach również z niego można korzystać. W nich (w odróżnieniu do DataFrame) dane składowane są bez porządku jaki dostarcza schemat danych. "
      ],
      "metadata": {
        "id": "xo581pEdh_du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tworzenie RDD ręcznie\n",
        "%%time\n",
        "sc = spark.sparkContext\n",
        "\n",
        "data = sc.parallelize(['A', 'B', 'C'])"
      ],
      "metadata": {
        "id": "rf4cVyFdiZmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data.collect()"
      ],
      "metadata": {
        "id": "uLdOMDpBQlzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby uzyskać dostęp do danych RDD można wydobyć je z DataFrame"
      ],
      "metadata": {
        "id": "o9uJ0DFORhBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_data = df.rdd\n",
        "rdd_data_sample = sc.parallelize(df.rdd.take(1000))"
      ],
      "metadata": {
        "id": "QqrA8W4ORPb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "rdd_data.take(5)"
      ],
      "metadata": {
        "id": "VSq5D1cYR5k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_data_sample.map(lambda row: row[4]+row[5]).collect()"
      ],
      "metadata": {
        "id": "Z8ZpscDmTGLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_data_sample.filter(lambda row: row[4] == 'DL').collect()"
      ],
      "metadata": {
        "id": "fnz2oGScXTJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_data_sample.flatMap(lambda row: row[4]+row[5]).collect()"
      ],
      "metadata": {
        "id": "-zQhfXfcYOa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_data_sample.map(lambda row: row[4])\\\n",
        "  .distinct()\\\n",
        "  .collect()"
      ],
      "metadata": {
        "id": "lmV-gaHrYVaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rdd_data_sample.sample(False,0.01).collect()"
      ],
      "metadata": {
        "id": "TzecglwmYhvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* .leftOuterJoin\n",
        "* .intersection\n",
        "* .repartition\n",
        "\n",
        "Akcje RDD\n",
        "\n",
        "* .take\n",
        "* .takeSample\n",
        "* .collect\n",
        "* .reduce\n",
        "* .count\n",
        "* .saveAsTextFile\n",
        "* .foreach"
      ],
      "metadata": {
        "id": "IjdJCEgsbsMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Przygotowanie Dataframe z pełnym schematem danych\n",
        "\n",
        "Poprzednio poszliśmy na skróty przypisując każdej ze zmiennych typ ciągu znaków. Tym razem zróbmy to porządnie"
      ],
      "metadata": {
        "id": "x9m-20U8-Ws0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "df.limit(5).toPandas()"
      ],
      "metadata": {
        "id": "8kh2aBAxY4oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StringType, IntegerType, BooleanType, FloatType, TimestampType, DateType, ArrayType, MapType\n",
        "from typing import List, Tuple, Dict, Any\n",
        "map_python_types_2_spark_types = {\n",
        "    str : StringType(),\n",
        "    int : IntegerType(),\n",
        "    bool : BooleanType(),\n",
        "    float: FloatType(),\n",
        "    'timestamp' : TimestampType(),\n",
        "    'date' : DateType(),\n",
        "    List[str] : ArrayType(StringType()),\n",
        "    Tuple[str] : ArrayType(StringType()),\n",
        "    Dict[str, str] : MapType(StringType(), StringType())\n",
        "}\n",
        "\n",
        "column_type_collection = {\n",
        "    int : ['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'DEPARTURE_DELAY', 'TAXI_OUT', 'SCHEDULED_TIME', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'TAXI_IN', 'ARRIVAL_DELAY', 'DIVERTED', 'CANCELLED' ],\n",
        "    str : ['AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'SCHEDULED_DEPARTURE', 'DEPARTURE_TIME', 'WHEELS_OFF', 'WHEELS_ON', \n",
        "      'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY'\n",
        "    ]\n",
        "}\n",
        "\n",
        "map_column_names_2_types = {}\n",
        "\n",
        "for pole in pola:\n",
        "  for python_type, column_list in column_type_collection.items():\n",
        "    if pole in column_list:\n",
        "      map_column_names_2_types[pole] = map_python_types_2_spark_types[python_type]\n",
        "\n",
        "print(map_column_names_2_types)\n",
        "\n"
      ],
      "metadata": {
        "id": "utF469t-_rBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schemat = StructType()\n",
        "for pole, typ in map_column_names_2_types.items():\n",
        "    schemat = schemat.add(pole, typ, True)\n"
      ],
      "metadata": {
        "id": "Tt02nPbaJdDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights = spark.read.format('csv').option(\"header\", False).schema(schemat).load('/content/drive/MyDrive/flights.csv')\n",
        "flights.show(5)"
      ],
      "metadata": {
        "id": "GdbdPAtoFs3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.printSchema()"
      ],
      "metadata": {
        "id": "9RBkfpCkMECV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.select(flights.DAY).distinct().show()"
      ],
      "metadata": {
        "id": "r3VaJTnqMGYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wykonajmy jeszcze proste statystyki ze zbioru by zobaczyć poprawność jego wczytania"
      ],
      "metadata": {
        "id": "miwuxu1OPlSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "flights.count()"
      ],
      "metadata": {
        "id": "Cdokj7MIPLSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for pole in ['YEAR', 'MONTH', 'DAY', 'AIRLINE', 'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DIVERTED', 'CANCELLED']:\n",
        "  print(f'Pole {pole}\\n')\n",
        "  print(flights.select(pole).distinct().sort(pole).toPandas())"
      ],
      "metadata": {
        "id": "r4WGdOEMPyZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.createOrReplaceTempView(\"flights\")"
      ],
      "metadata": {
        "id": "ef2HqdbRQL2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ćwiczenia \n",
        "\n",
        "Zadanie w ćwiczeniu polegać będą na stworzeniu konwersji zapytania Spark-SQL na składnię pyspark"
      ],
      "metadata": {
        "id": "NDo124nHS2C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select count(*) from flights where cancelled = 1').toPandas()"
      ],
      "metadata": {
        "id": "enimtWL-SY_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFQMg8AfSmS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select count(*) from flights where diverted = 1').toPandas()"
      ],
      "metadata": {
        "id": "TujMdkUdTdLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rbDaHCxTg8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select count(*) from flights where cancelled = 1 and diverted = 1').toPandas()"
      ],
      "metadata": {
        "id": "1wWILZTvThot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1_cByaUTkJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select avg(distance) from flights').toPandas()"
      ],
      "metadata": {
        "id": "cCW6bakMTzd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cir1oeHEUBfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select min(DEPARTURE_DELAY), max(DEPARTURE_DELAY) from flights').toPandas()"
      ],
      "metadata": {
        "id": "EJKOS3x1UEO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LzQZiAGHZLcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select min(ARRIVAL_DELAY), max(ARRIVAL_DELAY) from flights').toPandas()"
      ],
      "metadata": {
        "id": "HYXEOkFPUYK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-SGkcVeZMAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mieszanie składni SparkSQL i pyspark oraz praca z wieloma zbiorami danych\n",
        "\n",
        "Zagadnienie związane z tym która ze składni jest bardziej odpowiednia to dyskusja która nie jako toczy się od początku istnienia jej dualizmu w pyspark.\n",
        "Należy mieć na uwadze, że niezależnie od tego na który ze sposobów wyrazimy swoją kwerendę - spark i tak przetłumaczy ją na ciąg operacji na RDD i odpowiednio skolejkuje. Oznacza, że w każdej niemal sytuacji zadanie kwerendy poprzez formułę SQL oraz pysparka oznacza wykonanie tych samych operacji. Pozostawia to miejsce na używanie tej składni według preferencji użytkownika. Ale jednak każda z tych składni wnosi pewien narzut.\n",
        "\n",
        "Za składnią SQL przemawiają następujące argumenty:\n",
        "\n",
        "* Jest prostsza i często dużo łatwiej jest napisać kwerendę,\n",
        "* Zyskujemy na czasie pisania kwerendy.\n",
        "\n",
        "Za składnią pysparka natomiast:\n",
        "\n",
        "* zapisywanie kwerend w postaci funkcji pysparka pozwala lepiej zrozumieć kolejność operacji i dbałość o zmniejszanie obciążenia,\n",
        "* mamy możliwość reagowania na różnych etapach działania. Możemy tworzyć naszą kwerendę etapami i obserwować jej rozwój.\n",
        "* jeśli mamy wiele różnych zaawansowanych kwerend o wspólnej bazie to szybciej napiszemy je w pyspark\n",
        "* pyspark pozwala nam dużo wygodniej zarządzać tworzeniem kolumn\n",
        "\n"
      ],
      "metadata": {
        "id": "zr_Mlg2mafBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Praca z tworzeniem kolumn w pyspark\n",
        "\n",
        "Załóżmy, że dla naszych danych chcemy teraz utworzyć kolumną z wyliczonym na podstawie czasu przylotu i odlotu czasem. Pamiętamy, że część kolumn nie udała się zrzutować w czasie ładowania. Mamy zatem w bazie ciągi znaków jak 0815 odpowiadające godzinie 8:15, oraz inne które odpowiadają datom w sposób zrzutowany"
      ],
      "metadata": {
        "id": "XSwzpOSIJgvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_flights = df.select('year', 'month', 'day', 'airline', 'flight_number', 'tail_number', 'scheduled_departure', 'scheduled_time', 'scheduled_arrival')\n",
        "time_flights.limit(5).toPandas()"
      ],
      "metadata": {
        "id": "XJ2MbT66I0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import substring, col, expr\n",
        "time_flights2 = time_flights.withColumn('scheduled_departure_in_hours_str', substring('scheduled_departure',1,2))\n",
        "time_flights2 = time_flights2.withColumn('scheduled_departure_in_hours', col('scheduled_departure_in_hours_str').cast('integer'))\n",
        "time_flights2 = time_flights2.withColumn('scheduled_departure_in_minutes_str', substring('scheduled_departure',3,4))\n",
        "time_flights2 = time_flights2.withColumn('scheduled_departure_in_minutes', col('scheduled_departure_in_minutes_str').cast('integer'))\n",
        "time_flights2 = time_flights2.withColumn('scheduled_departure_in_minutes_from_midnight',expr('scheduled_departure_in_hours*60 + scheduled_departure_in_minutes '))\n",
        "time_flights2 = time_flights2.drop(\n",
        "    'scheduled_departure_in_hours_str','scheduled_departure_in_minutes_str', 'scheduled_departure_in_minutes','scheduled_departure_in_hours')\n",
        "time_flights2.limit(60).toPandas()"
      ],
      "metadata": {
        "id": "fdwXWclfKE51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_flights3 = time_flights2.withColumn('scheduled_arrival_in_hours_str', substring('scheduled_arrival',1,2))\n",
        "time_flights3 = time_flights3.withColumn('scheduled_arrival_in_hours', col('scheduled_arrival_in_hours_str').cast('integer'))\n",
        "time_flights3 = time_flights3.withColumn('scheduled_arrival_in_minutes_str', substring('scheduled_arrival',3,4))\n",
        "time_flights3 = time_flights3.withColumn('scheduled_arrival_in_minutes', col('scheduled_arrival_in_minutes_str').cast('integer'))\n",
        "time_flights3 = time_flights3.withColumn('scheduled_arrival_in_minutes_from_midnight',expr('scheduled_arrival_in_hours*60 + scheduled_arrival_in_minutes '))\n",
        "time_flights3 = time_flights3.drop(\n",
        "    'scheduled_arrival_in_hours_str','scheduled_arrival_in_minutes_str', 'scheduled_arrival_in_minutes','scheduled_arrival_in_hours')\n",
        "time_flights3.limit(60).toPandas()"
      ],
      "metadata": {
        "id": "apo8oiy-N31R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_flights3.where('scheduled_departure_in_minutes_from_midnight > scheduled_arrival_in_minutes_from_midnight').toPandas()"
      ],
      "metadata": {
        "id": "gYjJWARpZl90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_flights4 = time_flights3.withColumn('flight_time_diff',expr('scheduled_departure_in_minutes_from_midnight+scheduled_time-scheduled_arrival_in_minutes_from_midnight'))\n",
        "time_flights4.select('flight_time_diff').distinct().sort('flight_time_diff').toPandas()"
      ],
      "metadata": {
        "id": "LV8H8tl_Z4x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_flights4.where('flight_time_diff >= 1000').select('scheduled_time', 'flight_time_diff', 'scheduled_departure', 'scheduled_arrival').limit(100).toPandas()"
      ],
      "metadata": {
        "id": "egHl18hhoUWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_flights4.where('flight_time_diff < 1000').select('scheduled_time', 'flight_time_diff', 'scheduled_departure', 'scheduled_arrival').limit(100).toPandas()"
      ],
      "metadata": {
        "id": "JvHxu1UopKAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tworzenie zmiennych okienkowych\n",
        "\n",
        "Troszeczkę inaczej wygląda zadanie w którym chcemy przeprowadzić jakieś obliczenia związane ze zmiennymi reprezentowanymi przez tak zwane okna. Okna to przedziały (z reguły czasu) w których obliczane są statystyki dla poszczególnych elementów celem składowania ich w danych.\n",
        "\n",
        "Wiemy np. niektóre kierunki są popularniejsze od innych. Możemy spróbować pogrupować nasze dane tak aby zobaczyć na które lotniska w ostatnim przedziale czasowym przyleciało najwięcej samolotów. Kiedy chcemy aby dotyczyło to pojedynczego dnia zadanie jest proste."
      ],
      "metadata": {
        "id": "VmJhKV_YprrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "popular_airports = flights.groupBy('Year', 'Month', 'Day', 'Destination_airport').count().orderBy(col('count').desc())\n",
        "popular_airports.limit(20).toPandas()"
      ],
      "metadata": {
        "id": "bDus56FDpr2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.where('Month = 11 and day = 29 and destination_airport = \"ATL\" ').toPandas()"
      ],
      "metadata": {
        "id": "jWbocoEas8xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Troche trudniej sprawa wygląda jeśli podsumowanie ma dotyczyć np. ostatniego tygodnia.\n",
        "\n",
        "Najpierw wygenerujemy tabelę dziennych lotów na poszczególne dni"
      ],
      "metadata": {
        "id": "gBdg7w9ZtRpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "popular_airports = flights.groupBy('Year', 'Month', 'Day', 'Destination_airport').count().orderBy(col('count').desc())\n",
        "popular_airports.limit(10).toPandas()"
      ],
      "metadata": {
        "id": "bxwIR4FI3Kla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalej musimy ją przetworzyć tak aby policzyć większe okna czasowe"
      ],
      "metadata": {
        "id": "2fv_6LDC3VWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import concat_ws, to_timestamp, to_date, rank, count, sum\n",
        "\n",
        "days = lambda x: x * 86400\n",
        "\n",
        "destinations = popular_airports\n",
        "destinations = destinations.withColumn('Year_str', col('Year').cast('string') )\n",
        "destinations = destinations.withColumn('Month_str', col('Month').cast('string') )\n",
        "destinations = destinations.withColumn('Day_str', col('Day').cast('string') )\n",
        "destinations = destinations.withColumn('Date_str', concat_ws('-', 'Day_str', 'Month_str', 'Year_str') )\n",
        "destinations = destinations.withColumn('Date', to_timestamp('Date_str', 'd-M-yyyy' ))\n",
        "destinations = destinations.drop( 'Year_str', 'Month_str', 'Day_str', 'Date_str') #'Year','Month','Day',\n",
        "windSpec = Window.partitionBy('Destination_Airport').orderBy(col('Date').cast('long')).rangeBetween(-days(7),0)\n",
        "destinations = destinations.withColumn('Cumulative_flights', sum(col('count')).over(windSpec))\n",
        "destinations = destinations.drop('count')\n",
        "windSpec2 = Window.partitionBy('Date').orderBy(col('Cumulative_flights').desc())\n",
        "destinations = destinations.withColumn('Rank' , rank().over(windSpec2))\n",
        "destinations = destinations.drop('Date', 'Cumulative_flights')\n",
        "destinations.limit(100).toPandas()"
      ],
      "metadata": {
        "id": "yVuxEJ5wtSpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(f'flights size ')\n",
        "flights = flights.alias(\"left\").join(destinations.alias('right'), (flights.YEAR == destinations.Year)  & (flights.MONTH == destinations.Month) \n",
        "& (flights.DAY == destinations.Day)  & (flights.DESTINATION_AIRPORT == destinations.Destination_airport)).select(\"left.*\", 'right.rank') # "
      ],
      "metadata": {
        "id": "40JGu9b18Gyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.limit(5).toPandas()"
      ],
      "metadata": {
        "id": "8EZ5GqrP-qI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Można jeszcze użyć operacji `withColumnRenamed` do uporządkowania nazw kolumn"
      ],
      "metadata": {
        "id": "s4Ghj7zStRws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ćwiczenie warsztatowe\n",
        "\n",
        "## Zadanie 1 \n",
        "\n",
        "Utworzyć podzbiór zawierający tylko loty, które się odbyły. Znaleźć lot najkrótszy oraz najdłuższy.\n",
        "\n",
        "## Zadanie 2\n",
        "\n",
        "Wyszukać liczbę przewozników w danych i znaleźć łączną liczbę lotów wykonanych przez każdego z nich\n",
        "\n",
        "## Zadanie 3\n",
        "\n",
        "Dla każdej trasy (Lotnisko początkowe -> Lotnisko końcowe) znaleźć minimalny, przeciętny i maksymalny (rzeczywisty) czas przelotu\n",
        "\n",
        "## Zadanie 4\n",
        "\n",
        "Utworzyć nową kolumnę opisującą trasę (Lotnisko początkowe -> Lotnisko końcowe). Następnie sprawdzić czy w danych podanych jest zgodna odległość jego łączące\n",
        "\n",
        "## Zadanie 5\n",
        "\n",
        "Wygenerować tabelę z popularnością poszczególnych przewoźników na podstawie całego zbioru danych, wygenerować ich ranking i dołączyć go do danych flights jako kolumnę AIRLINE_RANK.\n"
      ],
      "metadata": {
        "id": "81L0DPOXpkUo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrJkYWdEpZMm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}